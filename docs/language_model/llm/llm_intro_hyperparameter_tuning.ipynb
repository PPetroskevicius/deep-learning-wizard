{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87a8028-ce51-45e6-9451-65793e6d1324",
   "metadata": {},
   "source": [
    "# LLM Introduction & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3c3d5-cfd6-4719-a7c3-a9e67ca25807",
   "metadata": {},
   "source": [
    "In this tutorial, we will be covering LLMs leveraging on Ollama and LlamaIndex using Gemma:7b (Google) open-source model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9725c36-bd64-444a-ade4-42a38119b365",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13995d-0b3a-468d-8a7c-c6f36969e78c",
   "metadata": {},
   "source": [
    "Follow our [tutorial on Apptainer](https://www.deeplearningwizard.com/language_model/containers/hpc_containers_apptainer/) to get started. Once you have followed the tutorial and you completed the [Ollama, LlamaIndex and Gemma:7b](https://www.deeplearningwizard.com/language_model/containers/hpc_containers_apptainer/#ollama-gemma-workload section), you will be able to run `jupyter lab` in a new window to access and run this notebook.\n",
    "\n",
    "!!! info  \"Directory Guide\"\n",
    "\n",
    "    When you shell into the Apptainer `.sif` container, you will need to navigate the directory as you normally would into the Deep Learning Wizard repository that you cloned, requiring you to `cd ..` to go back a few directories and finally reaching the right folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49fee1-0e46-40b0-ba4b-0d0cc5ea460a",
   "metadata": {},
   "source": [
    "## Question and Answer | No Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c9936-7da7-43e6-b42d-1907b16a3f62",
   "metadata": {},
   "source": [
    "In this section, we will leverage on the `Gemma:7b` LLM model to ask basic questions to get responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2e169-c225-406a-8eb2-33f001704587",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbdc4de1-24be-4e3d-a4fa-483a4f3cd901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore is a city-state located in Southeast Asia. It is a wealthy and cosmopolitan city known for its modern architecture, efficient public transportation system, and world-class shopping malls.\n"
     ]
    }
   ],
   "source": [
    "# Import the Ollama class from the llama_index.llms module.\n",
    "from llama_index.llms import Ollama\n",
    "\n",
    "# Create an instance of the Ollama class. The \"gemma:7b\" argument specifies the model to be used.\n",
    "llm = Ollama(model=\"gemma:7b\")\n",
    "\n",
    "# Call the complete method on the Ollama instance. \n",
    "# The method generates a completion for the given prompt \"What is Singapore?\".\n",
    "response = llm.complete(\"What is Singapore?\")\n",
    "\n",
    "# Print the generated response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35886d84-f261-4309-bfa8-b324f80aef44",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee2341f-da02-4b10-86ea-fac4676ac4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Large Language Models (LLMs)** are a type of language model that are trained on massive amounts of text data, typically billions or even trillions of words. LLMs are designed to understand and generate human-like text, and they are often used for a wide range of tasks, including:\n",
      "\n",
      "* **Text summarization:** Converting large amounts of text into shorter summaries.\n",
      "* **Text generation:** Generating new text, such as articles, stories, or code.\n",
      "* **Translation:** Translating text from one language to another.\n",
      "* **Question answering:** Answering questions based on text.\n",
      "* **Code generation:** Generating code in various programming languages.\n",
      "\n",
      "**Key Characteristics of LLMs:**\n",
      "\n",
      "* **Large-scale training:** LLMs are trained on massive datasets, typically billions or trillions of words.\n",
      "* **Text-centric:** LLMs are primarily designed to understand and generate text.\n",
      "* **Multi-task learning:** LLMs can be trained to perform multiple tasks, such as text summarization, translation, and code generation.\n",
      "* **Transfer learning:** LLMs can be fine-tuned for specific tasks, leveraging their general language understanding abilities.\n",
      "* **Human-like text generation:** LLMs can generate text that resembles human writing, often with high accuracy.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "* **GPT-3:** A popular LLM that is known for its ability to generate high-quality text.\n",
      "* **GPT-Neo:** A variant of GPT-3 that is designed for text generation tasks.\n",
      "* **Transformer-XL:** A large language model that is based on the transformer architecture.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "LLMs have a wide range of potential applications, including:\n",
      "\n",
      "* Text and code editing\n",
      "* Content creation\n",
      "* Information retrieval\n",
      "* Machine translation\n",
      "* Code development\n",
      "* Customer service\n",
      "\n",
      "**Note:** LLMs are still under development, and their capabilities are constantly evolving.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"What is a Large Language Model?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3a912-7f5f-44c6-aa9d-361ef008880c",
   "metadata": {},
   "source": [
    "In our second question, we change the question to \"What is a Large Language Model?\" and you can observe how the answer is substantially longer than the first question \"What is Singapore\". In the next section, you will discover that this relates to a few hyperparemeters in LLMs that can be tweaked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81c66b-2608-42c9-8ff1-fca95e44c97f",
   "metadata": {},
   "source": [
    "## Question and Answer | Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df2c30-a498-4661-a37e-ae19741fa6ba",
   "metadata": {},
   "source": [
    "### Temperature Tuning\n",
    "\n",
    "The `temperature` parameter in LLMs plays a pivotal role in determining the predictability of the output. \n",
    "\n",
    "- **Lower temperature values (e.g., 0.2)** lead to more predictable and consistent responses, but may risk being overly constrained or repetitive.\n",
    "\n",
    "- **Higher temperature values (e.g., 1.0)** introduce more randomness and diversity, but can result in less consistency and occasional incoherence.\n",
    "The choice of temperature value is a trade-off between consistency and variety, and should be tailored to the specific requirements of your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d96d0-4f97-462e-9ae0-d9e234146574",
   "metadata": {},
   "source": [
    "#### Low Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e42a33fd-aed7-4717-97f3-1f3a68014d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Birthday, [Friend's name]! I hope your day is filled with joy, laughter, and happiness! ðŸŽ‰ðŸŽ‚\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic\n",
    "temperature = 0.6\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt, temperature=temperature)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13872448-26df-4908-aa96-1887fe550727",
   "metadata": {},
   "source": [
    "#### High Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7417e317-6374-4551-9072-bd6c02a8c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are some happy birthday messages you can send to your friend:\n",
      "\n",
      "**Simple and sweet:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! Wishing you a day filled with joy and happiness.\"\n",
      "* \"Have a very happy birthday, [friend's name]! Sending positive vibes your way.\"\n",
      "* \"Wishing you a happy, happy birthday, [friend's name]! I hope it's a day full of treats.\"\n",
      "\n",
      "**Funny:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I'm not going to reveal how old you are, but I will say you're definitely not a teenager.\"\n",
      "* \"Hope your birthday is as awesome as you are, [friend's name]. I'm not kidding.\"\n",
      "* \"Have a birthday as bright as your smile, [friend's name]. I'm not a very creative person, but I tried.\"\n",
      "\n",
      "**Personal:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I'm so glad to have you in my life.\"\n",
      "* \"I'm wishing you a very happy birthday, [friend's name]. I hope your day is filled with all your favorite things.\"\n",
      "* \"I know it's your birthday today, [friend's name]. I'm sending you lots of love and best wishes.\"\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic\n",
    "temperature = 1.0\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt, temperature=temperature)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd96d5-45f1-49ae-9147-a2f709aaf2de",
   "metadata": {},
   "source": [
    "#### Mathematical Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58d958-c6aa-4976-bb95-accffa302706",
   "metadata": {},
   "source": [
    "In LLMs, the `temperature` parameter is used to control the randomness of predictions by scaling the logits before applying soft(arg)max.\n",
    "\n",
    "- The model computes a score (also known as logits) for each possible next token based on the current context. These scores are then transformed into probabilities using the soft(arg)max function.\n",
    "\n",
    "- The soft(arg)max function is defined as follows:\n",
    "\n",
    "    $$\\text{softargmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "- Before applying soft(arg)max, the logits are divided by the `temperature` value. This process is called temperature scaling and the equation becomes:\n",
    "\n",
    "    $$\\text{softargmax}(x_i) = \\frac{e^{x_i/T}}{\\sum_j e^{x_j/T}}$$\n",
    "\n",
    "- When `T` > 1, it makes the distribution more uniform (increases randomness). When `T` < 1, it makes the distribution more peaky (reduces randomness).\n",
    "\n",
    "So, in simple terms, a higher temperature value makes the model's output more random, and a lower temperature makes it more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d458879-dad7-4cf6-b396-6fbded6f52a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1\n",
      "[0.08714432 0.23688282 0.0320586  0.64391426]\n",
      "\n",
      "Temperature: 1.0\n",
      "[0.23632778 0.26118259 0.21383822 0.28865141]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softargmax(x, T=1.0):\n",
    "    e_x = np.exp(x / T)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Define logits\n",
    "logits = np.array([0.2, 0.3, 0.1, 0.4])\n",
    "\n",
    "# Compute soft(arg)max for different temperatures\n",
    "for T in [0.1, 1.0]:\n",
    "    print(f\"Temperature: {T}\")\n",
    "    print(softargmax(logits, T=T))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d80452-c074-4341-ae3d-168aadb913e5",
   "metadata": {},
   "source": [
    "In the Python code above leveraging on `numpy` library, you can see that\n",
    "\n",
    "- `softargmax` is a function that computes the soft(arg)max of an array of logits `x` for a given temperature `T`.\n",
    "- We define an array of logits and compute the soft(arg)max for different temperatures.\n",
    "- When you run this code, you'll see that as the temperature increases, the soft(arg)max output becomes more uniform (i.e., the probabilities are more evenly distributed), and as the temperature decreases, the soft(arg)max output becomes more peaky (i.e., one probability dominates the others). This illustrates how temperature can control the randomness of the model's output.\n",
    "\n",
    "- To close this off, taking the max of the soft(arg)max output, you will observe how it gets more random in the max value as the soft(arg)max output becomes more uniform. This links to the concept of how the next word gets more random because of the max of the uniformity of the soft(arg)max output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3e8bf-18b7-4351-8e1f-7fc33b7e676d",
   "metadata": {},
   "source": [
    "### Top-K Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9fe2f-3140-4e47-b968-6dc71a35a94b",
   "metadata": {},
   "source": [
    "In LLMs, the `top_k` hyperparameter is a key factor that influences the unpredictability of the generated output.\n",
    "\n",
    "- **For smaller `top_k` values**: The model behaves in a more predictable manner. It only takes into account a limited set of the most probable next tokens at each step of the generation process. This can result in responses that are more concise and consistent, but thereâ€™s a possibility that the output may be too restricted or repetitive.\n",
    "\n",
    "- **For larger `top_k` values**: The model takes into consideration a broader set of potential next tokens. This infuses more variety and randomness into the generated output. However, the responses can become less consistent and may occasionally be less coherent or pertinent.\n",
    "Therefore, the selection of the top_k value can be viewed as a balance between consistency and variety in the modelâ€™s responses. Itâ€™s crucial to adjust this parameter based on the specific needs of your task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec98395-34c0-4f9e-b092-0319038df82e",
   "metadata": {},
   "source": [
    "#### Low K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17c769a2-7fc0-422a-968e-0a865a066c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Birthday, [Friend's Name]! I hope your day is filled with joy, laughter, and happiness. May all your wishes come true! ðŸŽ‰ðŸŽ‚\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic\n",
    "temperature = 0.8\n",
    "\n",
    "# Set the top_k\n",
    "# This parameter controls the number of tokens considered for each step of the generation process\n",
    "top_k = 50\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt, temperature=temperature, top_k=top_k)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935896a-00a3-427e-87c0-0891930c680f",
   "metadata": {},
   "source": [
    "#### High K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7953aaa8-c950-4086-97f4-6f326ff5eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are a few options:\n",
      "\n",
      "**Short and sweet:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! Wishing you a day filled with joy and happiness.\"\n",
      "* \"Have a very happy birthday, [friend's name]! Let's celebrate!\"\n",
      "\n",
      "**More personal:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I hope your day is as special as you are.\"\n",
      "* \"Wishing you a very happy birthday, [friend's name]. I'm so glad to have you in my life.\"\n",
      "\n",
      "**Funny:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I'm not gonna lie, I'm a little jealous of your awesome day.\"\n",
      "* \"Have a truly happy birthday, [friend's name]. I'm not sure if you're actually going to be happy, but I'm hoping for your sake.\"\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic\n",
    "temperature = 0.8\n",
    "\n",
    "# Set the top_k\n",
    "# This parameter controls the number of tokens considered for each step of the generation process\n",
    "top_k = 100\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt, temperature=temperature, top_k=top_k)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccc614-bb2a-444f-878d-ade4af485064",
   "metadata": {},
   "source": [
    "You can observe that the reply is more diverse with a high `top_k` hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83daa3-259f-46ac-bccd-3e6c81feb661",
   "metadata": {},
   "source": [
    "#### Mathematical Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09f284-12c8-4022-aca6-e7bdb03fcd01",
   "metadata": {},
   "source": [
    "In LLMs, the `top_k` parameter is used to limit the number of next tokens considered for generation.\n",
    "\n",
    "- After computing the soft(arg)max probabilities for all possible next tokens, the model sorts these probabilities in descending order.\n",
    "\n",
    "- The model then only considers the `top_k` tokens with the highest probabilities for the next step of the generation process.\n",
    "\n",
    "- This process is called `top_k` sampling.\n",
    "\n",
    "Here's a simple Python code snippet that illustrates how `top_k` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "461b29c9-2dbc-4e3d-8256-befc38789af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 logits:\n",
      "[0.3 0.4]\n",
      "\n",
      "Top 3 logits:\n",
      "[0.2 0.3 0.4]\n",
      "\n",
      "Top 4 logits:\n",
      "[0.1 0.2 0.3 0.4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_k(logits, k):\n",
    "    # Sort the logits\n",
    "    sorted_indices = np.argsort(logits)\n",
    "    \n",
    "    # Consider only the top k\n",
    "    top_k_indices = sorted_indices[-k:]\n",
    "    \n",
    "    # Create a new array with only the top k probabilities\n",
    "    top_k_logits = logits[top_k_indices]\n",
    "    \n",
    "    return top_k_logits\n",
    "\n",
    "# Define logits\n",
    "logits = np.array([0.2, 0.3, 0.1, 0.4])\n",
    "\n",
    "# Compute top_k for different values of k\n",
    "for k in [2, 3, 4]:\n",
    "    print(f\"Top {k} logits:\")\n",
    "    print(top_k(logits, k=k))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f3191-2721-442b-9006-e36ab3e909b4",
   "metadata": {},
   "source": [
    "In the code above\n",
    "\n",
    "- `top_k` is a function that computes the top `k` logits from an array of logits.\n",
    "\n",
    "- We define an array of logits and compute the top `k` logits for different values of `k`.\n",
    "\n",
    "- When you run this code, you'll see that as `k` increases, more logits are considered. This illustrates how `top_k` can control the number of tokens considered by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77999932-0a70-4628-8abc-468e26d7689c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We covered the functionality of a basic LLM without any hyperparameter tuning. We then covered Temperature and Top-K hyperparameter tuning. It is important to note that there are many hyperparameters that can be tuned, and we will update this tutorial to gradually include as many as we can."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
