{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87a8028-ce51-45e6-9451-65793e6d1324",
   "metadata": {},
   "source": [
    "# LLM Introduction & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3c3d5-cfd6-4719-a7c3-a9e67ca25807",
   "metadata": {},
   "source": [
    "In this tutorial, we will be covering LLMs leveraging on Ollama and LlamaIndex using Gemma:7b (Google) open-source model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9725c36-bd64-444a-ade4-42a38119b365",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13995d-0b3a-468d-8a7c-c6f36969e78c",
   "metadata": {},
   "source": [
    "Follow our [tutorial on Apptainer](https://www.deeplearningwizard.com/language_model/containers/hpc_containers_apptainer/) to get started. Once you have followed the tutorial and you completed the [Ollama, LlamaIndex and Gemma:7b](https://www.deeplearningwizard.com/language_model/containers/hpc_containers_apptainer/#ollama-gemma-workload section), you will be able to run `jupyter lab` in a new window to access and run this notebook.\n",
    "\n",
    "!!! info  \"Directory Guide\"\n",
    "\n",
    "    When you shell into the Apptainer `.sif` container, you will need to navigate the directory as you normally would into the Deep Learning Wizard repository that you cloned, requiring you to `cd ..` to go back a few directories and finally reaching the right folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49fee1-0e46-40b0-ba4b-0d0cc5ea460a",
   "metadata": {},
   "source": [
    "## Question and Answer | No Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c9936-7da7-43e6-b42d-1907b16a3f62",
   "metadata": {},
   "source": [
    "In this section, we will leverage on the `Gemma:7b` LLM model to ask basic questions to get responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2e169-c225-406a-8eb2-33f001704587",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdc4de1-24be-4e3d-a4fa-483a4f3cd901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore is a city-state located on the island of Singapore. It is a Southeast Asian country and is known for its high standard of living, cleanliness, and efficiency.\n"
     ]
    }
   ],
   "source": [
    "# Import the Ollama class from the llama_index.llms.ollama module.\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Create an instance of the Ollama class. The \"gemma:7b\" argument specifies the model to be used.\n",
    "llm = Ollama(model=\"gemma:7b\")\n",
    "\n",
    "# Call the complete method on the Ollama instance. \n",
    "# The method generates a completion for the given prompt \"What is Singapore?\".\n",
    "response = llm.complete(\"What is Singapore?\")\n",
    "\n",
    "# Print the generated response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35886d84-f261-4309-bfa8-b324f80aef44",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee2341f-da02-4b10-86ea-fac4676ac4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is a type of language model that has been trained on a massive amount of text data, typically billions or trillions of words. LLMs are designed to be able to understand and generate human-like text, engage in natural language processing tasks, and provide information and knowledge across a wide range of topics. LLMs are typically deep learning models that are trained using transformer architectures, such as the GPT-3 model.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"What is a Large Language Model?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3a912-7f5f-44c6-aa9d-361ef008880c",
   "metadata": {},
   "source": [
    "In our second question, we change the question to \"What is a Large Language Model?\" and you can observe how the answer is substantially longer than the first question \"What is Singapore\". In the next section, you will discover that this relates to a few hyperparemeters in LLMs that can be tweaked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81c66b-2608-42c9-8ff1-fca95e44c97f",
   "metadata": {},
   "source": [
    "## Question and Answer | Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df2c30-a498-4661-a37e-ae19741fa6ba",
   "metadata": {},
   "source": [
    "### Temperature Tuning\n",
    "\n",
    "The `temperature` parameter in LLMs plays a pivotal role in determining the predictability of the output. \n",
    "\n",
    "- **Lower temperature values (e.g., 0.2)** lead to more predictable and consistent responses, but may risk being overly constrained or repetitive.\n",
    "\n",
    "- **Higher temperature values (e.g., 1.0)** introduce more randomness and diversity, but can result in less consistency and occasional incoherence.\n",
    "The choice of temperature value is a trade-off between consistency and variety, and should be tailored to the specific requirements of your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d96d0-4f97-462e-9ae0-d9e234146574",
   "metadata": {},
   "source": [
    "#### Low Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e42a33fd-aed7-4717-97f3-1f3a68014d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a happy birthday message for your friend:\n",
      "\n",
      "**Happy Birthday, [Friend's Name]!**\n",
      "\n",
      "I hope your day is filled with joy, laughter, and happiness. May all your wishes come true.\n",
      "\n",
      "Have a wonderful day, and I'm looking forward to celebrating with you soon.\n",
      "\n",
      "**Best regards,**\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.01) make it more deterministic\n",
    "temperature = 0.01\n",
    "# Instantiate the Ollama class again\n",
    "llm = Ollama(model=\"gemma:7b\", temperature=temperature)\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4650747-696f-448a-ab64-a44f8b1e3114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fd7e76754f0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7fd898237100>, completion_to_prompt=<function default_completion_to_prompt at 0x7fd8982bfd80>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='gemma:7b', temperature=0.01, context_window=3900, request_timeout=30.0, prompt_key='prompt', additional_kwargs={})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9686601-2c7d-46c6-831c-59eaf8f03188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Response 0\n",
      "----------\n",
      "Sure, here's a happy birthday message for your friend:\n",
      "\n",
      "**Happy Birthday, [Friend's Name]!**\n",
      "\n",
      "I hope your day is filled with joy, laughter, and happiness. May all your wishes come true.\n",
      "\n",
      "Have a wonderful day, and I'm looking forward to celebrating with you soon.\n",
      "\n",
      "**Best regards,**\n",
      "\n",
      "[Your Name]\n",
      "----------\n",
      "Response 1\n",
      "----------\n",
      "Sure, here's a happy birthday message for your friend:\n",
      "\n",
      "**Happy Birthday, [Friend's Name]!**\n",
      "\n",
      "I hope your day is filled with joy, laughter, and happiness. May all your wishes come true.\n",
      "\n",
      "Have a wonderful day, and I'm looking forward to celebrating with you soon.\n",
      "\n",
      "**Best regards,**\n",
      "\n",
      "[Your Name]\n",
      "----------\n",
      "Response 2\n",
      "----------\n",
      "Sure, here's a happy birthday message for your friend:\n",
      "\n",
      "**Happy Birthday, [Friend's Name]!**\n",
      "\n",
      "I hope your day is filled with joy, laughter, and happiness. May all your wishes come true.\n",
      "\n",
      "Have a wonderful day, and I'm looking forward to celebrating with you soon.\n",
      "\n",
      "**Best regards,**\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "# Run multiple times\n",
    "for i in range(3):\n",
    "    # Set the prompt\n",
    "    prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "    \n",
    "    # Set the temperature\n",
    "    # Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.01) make it more deterministic\n",
    "    temperature = 0.01\n",
    "    # Instantiate the Ollama class again\n",
    "    llm = Ollama(model=\"gemma:7b\", temperature=temperature)\n",
    "    \n",
    "    # Generate the response\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    # Print \n",
    "    print('-'*10)\n",
    "    print(f'Response {i}') \n",
    "    print('-'*10)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b3cba-f8b6-415f-8991-255e4df454d7",
   "metadata": {},
   "source": [
    "**We can see above it is almost the exact same answer calling the LLM 3 times**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13872448-26df-4908-aa96-1887fe550727",
   "metadata": {},
   "source": [
    "#### High Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7417e317-6374-4551-9072-bd6c02a8c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few happy birthday messages you can send to your friend:\n",
      "\n",
      "**Short and sweet:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! Wishing you a day filled with joy!\"\n",
      "* \"Have a very happy birthday, [friend's name]! Can't wait to see you!\"\n",
      "* \"Happy Birthday, [friend's name]! May your day be filled with happiness!\"\n",
      "\n",
      "**A little more personal:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I hope your day is as special as you are.\"\n",
      "* \"Have a wonderful birthday, [friend's name]! I'm so glad I have you in my life.\"\n",
      "* \"Wishing you a very happy birthday, [friend's name]. Let's celebrate this special day together!\"\n",
      "\n",
      "**Fun and cheeky:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I hope your day is filled with cake and laughter.\"\n",
      "* \"Have a great birthday, [friend's name]! I'm not going to tell you how old you are... for now, at least.\"\n",
      "* \"Happy Birthday, [friend's name]! May your day be filled with all your favorite things... even if it's me.\"\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.01) make it more deterministic\n",
    "temperature = 1.0\n",
    "# Instantiate the Ollama class again\n",
    "llm = Ollama(model=\"gemma:7b\", temperature=temperature)\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da0d7fd9-f562-40e0-a629-20323ebe08f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fd7e8b82ea0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7fd898237100>, completion_to_prompt=<function default_completion_to_prompt at 0x7fd8982bfd80>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='gemma:7b', temperature=1.0, context_window=3900, request_timeout=30.0, prompt_key='prompt', additional_kwargs={})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93888408-0818-43fb-b015-c60e8579f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Response 0\n",
      "----------\n",
      "Here are a few happy birthday messages you can send to your friend:\n",
      "\n",
      "**Classic Wishes:**\n",
      "\n",
      "* \"Happy Birthday, [Friend's Name]! Wishing you a day filled with joy, happiness, and laughter.\"\n",
      "* \"Have a very happy birthday, [Friend's Name]! May your day be filled with sunshine and good times.\"\n",
      "* \"Happy Birthday, my dear [Friend's Name]! I hope your day is as awesome as you are.\"\n",
      "\n",
      "**Personalized Wishes:**\n",
      "\n",
      "* \"Happy Birthday, [Friend's Name]! I hope your day is filled with [specific things you know your friend enjoys].\"\n",
      "* \"I'm so glad it's your birthday, [Friend's Name]! I'm sending you a virtual hug and a bunch of birthday wishes.\"\n",
      "* \"Wishing you a very happy birthday, [Friend's Name]! I can't wait to see what you have planned for this special day.\"\n",
      "\n",
      "**Fun and Quirky Wishes:**\n",
      "\n",
      "* \"Happy Birthday, [Friend's Name]! May your day be filled with cake and laughter... and maybe a sprinkle of unicorn magic.\"\n",
      "* \"Have a very happy birthday, [Friend's Name]! I'm hoping your day is as memorable as a trip to the moon.\"\n",
      "* \"Happy Birthday, [Friend's Name]! I'm sending you virtual balloons and a party hat big enough for the both of us.\"\n",
      "----------\n",
      "Response 1\n",
      "----------\n",
      "Here are some happy birthday messages you can send to your friend:\n",
      "\n",
      "**Classic wishes:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! May your day be filled with joy, laughter, and good times.\"\n",
      "* \"Have a very happy birthday, [friend's name]! I hope all your wishes come true.\"\n",
      "* \"Wishing you a very happy birthday, [friend's name]! I'm sending you warmest wishes for a day filled with happiness.\"\n",
      "\n",
      "**Personalized wishes:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I hope your day is as special as you are.\"\n",
      "* \"Have a wonderful birthday, [friend's name]! I'm so glad to have you in my life.\"\n",
      "* \"Sending you big birthday wishes, [friend's name]! I can't wait to see what you have planned.\"\n",
      "\n",
      "**Fun and cheesy:**\n",
      "\n",
      "* \"Happy Birthday, [friend's name]! I'm hoping you have a day as awesome as you are.\"\n",
      "* \"Have a blast on your birthday, [friend's name]! I'm planning on eating a cake in your honor.\"\n",
      "* \"I'm not a party pooper, but I'm definitely not attending your party, [friend's name]. Have a great day!\"\n",
      "\n",
      "**Remember:**\n",
      "\n",
      "* You can personalize the message with your friend's name and preferred gender-neutral pronouns.\n",
      "* You can add a specific wish or goal you have for your friend.\n",
      "* You can include a funny joke or a reference to a shared inside joke.\n",
      "* You can keep the message short and sweet, or you can write a longer, more heartfelt message.\n",
      "----------\n",
      "Response 2\n",
      "----------\n",
      "Sure, here's a happy birthday message you can send to your friend:\n",
      "\n",
      "**Happy Birthday, [Friend's Name]!**\n",
      "\n",
      "May your day be filled with joy, laughter, and happiness. I hope your special day is filled with all your favorite things, and I'm wishing you a very, very happy birthday!\n"
     ]
    }
   ],
   "source": [
    "# Run multiple times\n",
    "for i in range(3):\n",
    "    # Set the prompt\n",
    "    prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "    \n",
    "    # Set the temperature\n",
    "    # Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.01) make it more deterministic\n",
    "    temperature = 1.0\n",
    "    # Instantiate the Ollama class again\n",
    "    llm = Ollama(model=\"gemma:7b\", temperature=temperature)\n",
    "    \n",
    "    # Generate the response\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    # Print \n",
    "    print('-'*10)\n",
    "    print(f'Response {i}') \n",
    "    print('-'*10)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b684188-2490-4f29-ae2a-428810c61f28",
   "metadata": {},
   "source": [
    "**Here, you can see very varied answer in each of the 3 calls to the LLM commpared to lower temperature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd96d5-45f1-49ae-9147-a2f709aaf2de",
   "metadata": {},
   "source": [
    "#### Mathematical Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58d958-c6aa-4976-bb95-accffa302706",
   "metadata": {},
   "source": [
    "In LLMs, the `temperature` parameter is used to control the randomness of predictions by scaling the logits before applying soft(arg)max.\n",
    "\n",
    "- The model computes a score (also known as logits) for each possible next token based on the current context. These scores are then transformed into probabilities using the soft(arg)max function.\n",
    "\n",
    "- The soft(arg)max function is defined as follows:\n",
    "\n",
    "    $$\\text{softargmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "- Before applying soft(arg)max, the logits are divided by the `temperature` value. This process is called temperature scaling and the equation becomes:\n",
    "\n",
    "    $$\\text{softargmax}(x_i) = \\frac{e^{x_i/T}}{\\sum_j e^{x_j/T}}$$\n",
    "\n",
    "- When `T` > 1, it makes the distribution more uniform (increases randomness). When `T` < 1, it makes the distribution more peaky (reduces randomness).\n",
    "\n",
    "So, in simple terms, a higher temperature value makes the model's output more random, and a lower temperature makes it more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d458879-dad7-4cf6-b396-6fbded6f52a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1\n",
      "[0.08714432 0.23688282 0.0320586  0.64391426]\n",
      "\n",
      "Temperature: 1.0\n",
      "[0.23632778 0.26118259 0.21383822 0.28865141]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softargmax(x, T=1.0):\n",
    "    e_x = np.exp(x / T)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Define logits\n",
    "logits = np.array([0.2, 0.3, 0.1, 0.4])\n",
    "\n",
    "# Compute soft(arg)max for different temperatures\n",
    "for T in [0.1, 1.0]:\n",
    "    print(f\"Temperature: {T}\")\n",
    "    print(softargmax(logits, T=T))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d80452-c074-4341-ae3d-168aadb913e5",
   "metadata": {},
   "source": [
    "In the Python code above leveraging on `numpy` library, you can see that\n",
    "\n",
    "- `softargmax` is a function that computes the soft(arg)max of an array of logits `x` for a given temperature `T`.\n",
    "- We define an array of logits and compute the soft(arg)max for different temperatures.\n",
    "- When you run this code, you'll see that as the temperature increases, the soft(arg)max output becomes more uniform (i.e., the probabilities are more evenly distributed), and as the temperature decreases, the soft(arg)max output becomes more peaky (i.e., one probability dominates the others). This illustrates how temperature can control the randomness of the model's output.\n",
    "\n",
    "- To close this off, taking the max of the soft(arg)max output, you will observe how it gets more random in the max value as the soft(arg)max output becomes more uniform. This links to the concept of how the next word gets more random because of the max of the uniformity of the soft(arg)max output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3e8bf-18b7-4351-8e1f-7fc33b7e676d",
   "metadata": {},
   "source": [
    "### Top-K Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9fe2f-3140-4e47-b968-6dc71a35a94b",
   "metadata": {},
   "source": [
    "In LLMs, the `top_k` hyperparameter is a key factor that influences the unpredictability of the generated output.\n",
    "\n",
    "- **For smaller `top_k` values**: The model behaves in a more predictable manner. It only takes into account a limited set of the most probable next tokens at each step of the generation process. This can result in responses that are more concise and consistent, but there’s a possibility that the output may be too restricted or repetitive.\n",
    "\n",
    "- **For larger `top_k` values**: The model takes into consideration a broader set of potential next tokens. This infuses more variety and randomness into the generated output. However, the responses can become less consistent and may occasionally be less coherent or pertinent.\n",
    "Therefore, the selection of the top_k value can be viewed as a balance between consistency and variety in the model’s responses. It’s crucial to adjust this parameter based on the specific needs of your task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec98395-34c0-4f9e-b092-0319038df82e",
   "metadata": {},
   "source": [
    "#### Low K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17c769a2-7fc0-422a-968e-0a865a066c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Birthday, [Friend's Name]! I hope your day is filled with joy, happiness, and laughter. May all your wishes come true. 🎉🎂\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic\n",
    "temperature = 0.8\n",
    "\n",
    "# Set the top_k\n",
    "# This parameter controls the number of tokens considered for each step of the generation process\n",
    "top_k = 50\n",
    "\n",
    "# Instantiate the Ollama class again\n",
    "llm = Ollama(model=\"gemma:7b\", temperature=temperature, top_k=top_k)\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "476c6919-a41f-455e-a17b-4aef2f43940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fd7e7248950>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x7fd898237100>, completion_to_prompt=<function default_completion_to_prompt at 0x7fd8982bfd80>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://localhost:11434', model='gemma:7b', temperature=0.8, context_window=3900, request_timeout=30.0, prompt_key='prompt', additional_kwargs={})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935896a-00a3-427e-87c0-0891930c680f",
   "metadata": {},
   "source": [
    "#### High K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7953aaa8-c950-4086-97f4-6f326ff5eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Birthday, [Friend's Name]! I hope your day is filled with joy, laughter, and happiness. 🎉🎂\n"
     ]
    }
   ],
   "source": [
    "# Set the prompt\n",
    "prompt = \"Write a happy birthday message, I would like to send to my friend.\"\n",
    "\n",
    "# Set the temperature\n",
    "# Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.2) make it more deterministic\n",
    "temperature = 0.8\n",
    "\n",
    "# Set the top_k\n",
    "# This parameter controls the number of tokens considered for each step of the generation process\n",
    "top_k = 100\n",
    "\n",
    "# Generate the response\n",
    "response = llm.complete(prompt, temperature=temperature, top_k=top_k)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccc614-bb2a-444f-878d-ade4af485064",
   "metadata": {},
   "source": [
    "You can observe that the reply is more diverse with a high `top_k` hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83daa3-259f-46ac-bccd-3e6c81feb661",
   "metadata": {},
   "source": [
    "#### Mathematical Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09f284-12c8-4022-aca6-e7bdb03fcd01",
   "metadata": {},
   "source": [
    "In LLMs, the `top_k` parameter is used to limit the number of next tokens considered for generation.\n",
    "\n",
    "- After computing the soft(arg)max probabilities for all possible next tokens, the model sorts these probabilities in descending order.\n",
    "\n",
    "- The model then only considers the `top_k` tokens with the highest probabilities for the next step of the generation process.\n",
    "\n",
    "- This process is called `top_k` sampling.\n",
    "\n",
    "Here's a simple Python code snippet that illustrates how `top_k` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461b29c9-2dbc-4e3d-8256-befc38789af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 logits:\n",
      "[0.3 0.4]\n",
      "\n",
      "Top 3 logits:\n",
      "[0.2 0.3 0.4]\n",
      "\n",
      "Top 4 logits:\n",
      "[0.1 0.2 0.3 0.4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_k(logits, k):\n",
    "    # Sort the logits\n",
    "    sorted_indices = np.argsort(logits)\n",
    "    \n",
    "    # Consider only the top k\n",
    "    top_k_indices = sorted_indices[-k:]\n",
    "    \n",
    "    # Create a new array with only the top k probabilities\n",
    "    top_k_logits = logits[top_k_indices]\n",
    "    \n",
    "    return top_k_logits\n",
    "\n",
    "# Define logits\n",
    "logits = np.array([0.2, 0.3, 0.1, 0.4])\n",
    "\n",
    "# Compute top_k for different values of k\n",
    "for k in [2, 3, 4]:\n",
    "    print(f\"Top {k} logits:\")\n",
    "    print(top_k(logits, k=k))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f3191-2721-442b-9006-e36ab3e909b4",
   "metadata": {},
   "source": [
    "In the code above\n",
    "\n",
    "- `top_k` is a function that computes the top `k` logits from an array of logits.\n",
    "\n",
    "- We define an array of logits and compute the top `k` logits for different values of `k`.\n",
    "\n",
    "- When you run this code, you'll see that as `k` increases, more logits are considered. This illustrates how `top_k` can control the number of tokens considered by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77999932-0a70-4628-8abc-468e26d7689c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We covered the functionality of a basic LLM without any hyperparameter tuning. We then covered Temperature and Top-K hyperparameter tuning. It is important to note that there are many hyperparameters that can be tuned, and we will update this tutorial to gradually include as many as we can."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
