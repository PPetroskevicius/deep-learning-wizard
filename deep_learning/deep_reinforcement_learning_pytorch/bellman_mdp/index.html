
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="We try to make learning deep learning, deep bayesian learning, and deep reinforcement learning math and code easier. Used by thousands.">
      
      
      
        <meta name="author" content="Ritchie Ng">
      
      
        <link rel="canonical" href="https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/">
      
      <link rel="shortcut icon" href="../../../docs/assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.2.4">
    
    
      
        <title>Markov Decision Processes (MDP) and Bellman Equations - Deep Learning Wizard</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.15aa0b43.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.75751829.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-122083328-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#markov-decision-processes-mdps" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-header-nav__button md-logo" aria-label="Deep Learning Wizard">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M396.8 352h22.4c6.4 0 12.8-6.4 12.8-12.8V108.8c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v230.4c0 6.4 6.4 12.8 12.8 12.8zm-192 0h22.4c6.4 0 12.8-6.4 12.8-12.8V140.8c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v198.4c0 6.4 6.4 12.8 12.8 12.8zm96 0h22.4c6.4 0 12.8-6.4 12.8-12.8V204.8c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v134.4c0 6.4 6.4 12.8 12.8 12.8zM496 400H48V80c0-8.84-7.16-16-16-16H16C7.16 64 0 71.16 0 80v336c0 17.67 14.33 32 32 32h464c8.84 0 16-7.16 16-16v-16c0-8.84-7.16-16-16-16zm-387.2-48h22.4c6.4 0 12.8-6.4 12.8-12.8v-70.4c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v70.4c0 6.4 6.4 12.8 12.8 12.8z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      <div class="md-header-nav__ellipsis">
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            Deep Learning Wizard
          </span>
        </div>
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            
              Markov Decision Processes (MDP) and Bellman Equations
            
          </span>
        </div>
      </div>
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/ritchieng/deep-learning-wizard/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ritchieng/deep-learning-wizard
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../home/" class="md-tabs__link">
        About
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../intro/" class="md-tabs__link md-tabs__link--active">
        Deep Learning (CPU/GPU)
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../machine_learning/intro/" class="md-tabs__link">
        Machine Learning (CPU/GPU)
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../programming/intro/" class="md-tabs__link">
        Programming
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../ood/intro/" class="md-tabs__link">
        Out of Distribution Data
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../database/intro/" class="md-tabs__link">
        Scalable Database
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../news/news/" class="md-tabs__link">
        News
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://www.deeplearningwizard.com/" title="Deep Learning Wizard" class="md-nav__button md-logo" aria-label="Deep Learning Wizard">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M396.8 352h22.4c6.4 0 12.8-6.4 12.8-12.8V108.8c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v230.4c0 6.4 6.4 12.8 12.8 12.8zm-192 0h22.4c6.4 0 12.8-6.4 12.8-12.8V140.8c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v198.4c0 6.4 6.4 12.8 12.8 12.8zm96 0h22.4c6.4 0 12.8-6.4 12.8-12.8V204.8c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v134.4c0 6.4 6.4 12.8 12.8 12.8zM496 400H48V80c0-8.84-7.16-16-16-16H16C7.16 64 0 71.16 0 80v336c0 17.67 14.33 32 32 32h464c8.84 0 16-7.16 16-16v-16c0-8.84-7.16-16-16-16zm-387.2-48h22.4c6.4 0 12.8-6.4 12.8-12.8v-70.4c0-6.4-6.4-12.8-12.8-12.8h-22.4c-6.4 0-12.8 6.4-12.8 12.8v70.4c0 6.4 6.4 12.8 12.8 12.8z"/></svg>

    </a>
    Deep Learning Wizard
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ritchieng/deep-learning-wizard/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ritchieng/deep-learning-wizard
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      



  <li class="md-nav__item">
    <a href="../../.." class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" >
    
    <label class="md-nav__link" for="nav-2">
      About
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="About" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon"></span>
        About
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../../home/" class="md-nav__link">
      Intro
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../about/" class="md-nav__link">
      About Us
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../review/" class="md-nav__link">
      Reviews
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../pipeline/" class="md-nav__link">
      AI Pipeline
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../consultancy/" class="md-nav__link">
      Consultancy
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  


  
  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      Deep Learning (CPU/GPU)
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Deep Learning (CPU/GPU)" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon"></span>
        Deep Learning (CPU/GPU)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../intro/" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../course_progression/" class="md-nav__link">
      Course Progression
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_matrices/" class="md-nav__link">
      Matrices
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_gradients/" class="md-nav__link">
      Gradients
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_linear_regression/" class="md-nav__link">
      Linear Regression
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_logistic_regression/" class="md-nav__link">
      Logistic Regression
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_feedforward_neuralnetwork/" class="md-nav__link">
      Feedforward Neural Networks (FNN)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_convolutional_neuralnetwork/" class="md-nav__link">
      Convolutional Neural Networks (CNN)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_recurrent_neuralnetwork/" class="md-nav__link">
      Recurrent Neural Networks (RNN)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_lstm_neuralnetwork/" class="md-nav__link">
      Long Short Term Memory Neural Networks (LSTM)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_autoencoder/" class="md-nav__link">
      Autoencoders (AE)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../practical_pytorch/pytorch_fc_overcomplete_ae/" class="md-nav__link">
      Fully-connected Overcomplete Autoencoder (AE)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/derivative_gradient_jacobian/" class="md-nav__link">
      Derivative, Gradient and Jacobian
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/forwardpropagation_backpropagation_gradientdescent/" class="md-nav__link">
      Forward- and Backward-propagation and Gradient Descent (From Scratch FNN Regression)
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../fromscratch/fromscratch_logistic_regression/" class="md-nav__link">
      From Scratch Logistic Regression Classification
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../fromscratch/fromscratch_cnn/" class="md-nav__link">
      From Scratch CNN Classification
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/lr_scheduling/" class="md-nav__link">
      Learning Rate Scheduling
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/optimizers/" class="md-nav__link">
      Optimization Algorithms
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../boosting_models_pytorch/weight_initialization_activation_functions/" class="md-nav__link">
      Weight Initialization and Activation Functions
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../supervised_to_rl/" class="md-nav__link">
      Supervised Learning to Reinforcement Learning (RL)
    </a>
  </li>

        
          
          
          


  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Markov Decision Processes (MDP) and Bellman Equations
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      Markov Decision Processes (MDP) and Bellman Equations
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdps" class="md-nav__link">
    Markov Decision Processes (MDPs)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-main-characteristics-for-mdps" class="md-nav__link">
    Two main characteristics for MDPs
  </a>
  
    <nav class="md-nav" aria-label="Two main characteristics for MDPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-markov-models" class="md-nav__link">
    Types of Markov Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pomdps" class="md-nav__link">
    POMDPs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-components-of-mdps" class="md-nav__link">
    5 Components of MDPs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moving-from-mdps-to-optimal-policy" class="md-nav__link">
    Moving From MDPs to Optimal Policy
  </a>
  
    <nav class="md-nav" aria-label="Moving From MDPs to Optimal Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#categories-of-design" class="md-nav__link">
    Categories of Design
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policy" class="md-nav__link">
    Optimal Policy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation" class="md-nav__link">
    Bellman Equation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-recap-on-value-functions" class="md-nav__link">
    Key Recap on Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-expectation-equations" class="md-nav__link">
    Bellman Expectation Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-equations" class="md-nav__link">
    Bellman Optimality Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-action-value-and-state-value-functions" class="md-nav__link">
    Optimal Action-value and State-value functions
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../dynamic_programming_frozenlake/" class="md-nav__link">
      Dynamic Programming
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../production_pytorch/speed_optimization_basics_numba/" class="md-nav__link">
      Speed Optimization Basics Numba
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../readings/" class="md-nav__link">
      Additional Readings
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" >
    
    <label class="md-nav__link" for="nav-4">
      Machine Learning (CPU/GPU)
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Machine Learning (CPU/GPU)" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon"></span>
        Machine Learning (CPU/GPU)
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../../machine_learning/intro/" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../machine_learning/gpu/rapids_cudf/" class="md-nav__link">
      GPU DataFrames
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../machine_learning/gpu/gpu_fractional_differencing/" class="md-nav__link">
      GPU/CPU Fractional Differencing
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5" >
    
    <label class="md-nav__link" for="nav-5">
      Programming
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Programming" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon"></span>
        Programming
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../../programming/intro/" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../programming/cpp/cpp/" class="md-nav__link">
      C++
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../programming/bash/bash/" class="md-nav__link">
      Bash
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../programming/python/python/" class="md-nav__link">
      Python
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../programming/javascript/javascript/" class="md-nav__link">
      Javascript
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../programming/electron/electron/" class="md-nav__link">
      Electron
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6" >
    
    <label class="md-nav__link" for="nav-6">
      Out of Distribution Data
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Out of Distribution Data" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        <span class="md-nav__icon md-icon"></span>
        Out of Distribution Data
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../../ood/intro/" class="md-nav__link">
      Introduction
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7" >
    
    <label class="md-nav__link" for="nav-7">
      Scalable Database
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Scalable Database" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        <span class="md-nav__icon md-icon"></span>
        Scalable Database
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../../database/intro/" class="md-nav__link">
      Introduction
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../database/setting_up_cluster/" class="md-nav__link">
      Cassandra Cluster Setup
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      



  
  <li class="md-nav__item md-nav__item--nested">
    
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-8" type="checkbox" id="nav-8" >
    
    <label class="md-nav__link" for="nav-8">
      News
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="News" data-md-level="1">
      <label class="md-nav__title" for="nav-8">
        <span class="md-nav__icon md-icon"></span>
        News
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/news/" class="md-nav__link">
      Welcome
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/dbs_gpu_rapids_nvidia_ensemble_frac_diff/" class="md-nav__link">
      Fractional Differencing with GPU (GFD), DBS and NVIDIA, September 2019
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/defence_and_science_technology_agency_dsta_nvidia_talk_2016_06/" class="md-nav__link">
      Deep Learning Introduction, Defence and Science Technology Agency (DSTA) and NVIDIA, June 2019
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/detect_waterbone_debris_ai_for_social_good_icml_2019_06/" class="md-nav__link">
      Oral Presentation for AI for Social Good Workshop ICML, June 2019
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/it_youth_leader_2019_03_11/" class="md-nav__link">
      IT Youth Leader of The Year 2019, March 2019
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/ammi_facebook_google_recap_2018_11_21/" class="md-nav__link">
      AMMI (AIMS) supported by Facebook and Google, November 2018
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/nanjing_next_nus_tsinghua_ai_finance_healthcare_2018_11_01/" class="md-nav__link">
      NExT++ AI in Healthcare and Finance, Nanjing, November 2018
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_devcon_recap_2018_10_02/" class="md-nav__link">
      Recap of Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/facebook_pytorch_developer_conference_2018_09_05/" class="md-nav__link">
      Facebook PyTorch Developer Conference, San Francisco, September 2018
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/nvidia_nus_mit_datathon_2018_07_05/" class="md-nav__link">
      NUS-MIT-NUHS NVIDIA Image Recognition Workshop, Singapore, July 2018
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_1y_2018_06_01/" class="md-nav__link">
      Featured on PyTorch Website 2018
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/nvidia_self_driving_cars_talk_2017_06_21/" class="md-nav__link">
      NVIDIA Self Driving Cars & Healthcare Talk, Singapore, June 2017
    </a>
  </li>

        
          
          
          



  <li class="md-nav__item">
    <a href="../../../news/deep_learning_wizard_nvidia_inception_2018_05_01/" class="md-nav__link">
      NVIDIA Inception Partner Status, Singapore, May 2017
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdps" class="md-nav__link">
    Markov Decision Processes (MDPs)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#two-main-characteristics-for-mdps" class="md-nav__link">
    Two main characteristics for MDPs
  </a>
  
    <nav class="md-nav" aria-label="Two main characteristics for MDPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-markov-models" class="md-nav__link">
    Types of Markov Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pomdps" class="md-nav__link">
    POMDPs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-components-of-mdps" class="md-nav__link">
    5 Components of MDPs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#moving-from-mdps-to-optimal-policy" class="md-nav__link">
    Moving From MDPs to Optimal Policy
  </a>
  
    <nav class="md-nav" aria-label="Moving From MDPs to Optimal Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#categories-of-design" class="md-nav__link">
    Categories of Design
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-policy" class="md-nav__link">
    Optimal Policy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation" class="md-nav__link">
    Bellman Equation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-recap-on-value-functions" class="md-nav__link">
    Key Recap on Value Functions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-expectation-equations" class="md-nav__link">
    Bellman Expectation Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-equations" class="md-nav__link">
    Bellman Optimality Equations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-action-value-and-state-value-functions" class="md-nav__link">
    Optimal Action-value and State-value functions
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ritchieng/deep-learning-wizard/edit/master/docs/deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  <h1>Markov Decision Processes (MDP) and Bellman Equations</h1>
                
                <h2 id="markov-decision-processes-mdps">Markov Decision Processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Permanent link">&para;</a></h2>
<ul>
<li>Typically we can <strong>frame all RL tasks as MDPs</strong><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup><ul>
<li>Intuitively, it's sort of a way to frame RL tasks such that we can solve them in a "principled" manner. We will go into the specifics throughout this tutorial</li>
</ul>
</li>
<li>The key in MDPs is the <strong>Markov Property</strong><ul>
<li>Essentially the future depends on the present and not the past<ul>
<li>More specifically, the future is independent of the past given the present</li>
<li>There's an assumption the present state encapsulates past information. This is not always true, see the note below.</li>
</ul>
</li>
</ul>
</li>
<li>Putting into the context of what we have covered so far: our agent can (1) <strong>control its action</strong> based on its current (2) <strong>completely known state</strong><ul>
<li>Back to the "driving to avoid puppy" example: given we know there is a dog in front of the car as the current state and the car is always moving forward (no reverse driving), the agent can decide to take a left/right turn to avoid colliding with the puppy in front</li>
</ul>
</li>
</ul>
<h2 id="two-main-characteristics-for-mdps">Two main characteristics for MDPs<a class="headerlink" href="#two-main-characteristics-for-mdps" title="Permanent link">&para;</a></h2>
<ol>
<li>Control over state transitions</li>
<li>States completely observable</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Other Markov Models</p>
<p>Permutations of whether there is presence of the two main characteristics would lead to different Markov models. These are not important now, but it gives you an idea of what other frameworks we can use besides MDPs.</p>
<h6 id="types-of-markov-models">Types of Markov Models<a class="headerlink" href="#types-of-markov-models" title="Permanent link">&para;</a></h6>
<ol>
<li><ins class="critic">Control</ins> over state transitions and <ins class="critic">completely observable</ins> states: <strong>MDPs</strong></li>
<li><ins class="critic">Control</ins> over state transitions and <mark>partially observable states</mark>: <strong>Partially Observable MDPs (POMDPs)</strong></li>
<li><mark>No control</mark> over state transitions and <ins class="critic">completely observable</ins> states: <strong>Markov Chain</strong></li>
<li><mark>No control</mark> over state transitions and <mark>partially observable</mark> states: <strong>Hidden Markov Model</strong></li>
</ol>
<h6 id="pomdps">POMDPs<a class="headerlink" href="#pomdps" title="Permanent link">&para;</a></h6>
<ul>
<li>Imagine our driving example where we don't know if the car is going forward/backward in its state, but only know there is a puppy in the center lane in front, this is a partially observable state</li>
<li>There are ways to counter this<ol>
<li>Use the <strong>complete history</strong> to construct the current state</li>
<li>Represent the current state as a probability distribution (<strong>bayesian approach</strong>) of what the agent perceives of the current state</li>
<li>Using a <strong>RNN</strong> to form the current state that encapsulates the past<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup></li>
</ol>
</li>
</ul>
</div>
<h2 id="5-components-of-mdps">5 Components of MDPs<a class="headerlink" href="#5-components-of-mdps" title="Permanent link">&para;</a></h2>
<ol>
<li><span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span>: set of states</li>
<li><span><span class="MathJax_Preview">\mathcal{A}</span><script type="math/tex">\mathcal{A}</script></span>: set of actions</li>
<li><span><span class="MathJax_Preview">\mathcal{R}</span><script type="math/tex">\mathcal{R}</script></span>: reward function</li>
<li><span><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span>: transition probability function</li>
<li><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span>: discount for future rewards</li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Remembering 5 components with a mnemonic</p>
<p>A mnemonic I use to remember the 5 components is the acronym "SARPY" (sar-py). I know <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> is not <span><span class="MathJax_Preview">\mathcal{Y}</span><script type="math/tex">\mathcal{Y}</script></span> but it looks like a <code>y</code> so there's that.</p>
</div>
<h2 id="moving-from-mdps-to-optimal-policy">Moving From MDPs to Optimal Policy<a class="headerlink" href="#moving-from-mdps-to-optimal-policy" title="Permanent link">&para;</a></h2>
<ul>
<li>We have an <strong>agent</strong> acting in an <strong>environment</strong></li>
<li>The way the <strong>environment</strong> reacts to the agent's <strong>actions (<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>)</strong> is dictated by a <strong>model</strong></li>
<li>The agent can take <strong>actions (<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>)</strong> to move from one <strong>state (<span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>)</strong> to another <strong>new state (<span><span class="MathJax_Preview">s')</span><script type="math/tex">s')</script></span></strong></li>
<li>When the agent has transited to a <strong>new state (<span><span class="MathJax_Preview">s')</span><script type="math/tex">s')</script></span></strong>, there will a <strong>reward (<span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>)</strong></li>
<li>We may or may not know our <strong>model</strong><ol>
<li><strong>Model-based RL</strong>: this is where we can <ins class="critic">clearly define</ins> our (1) transition probabilities and/or (2) reward function<ul>
<li>A global minima can be attained via Dynamic Programming (DP)</li>
</ul>
</li>
<li><strong>Model-free RL</strong>: this is where we <mark>cannot clearly define</mark> our (1) transition probabilities and/or (2) reward function<ul>
<li>Most real-world problems are under this category so we will mostly place our attention on this category</li>
</ul>
</li>
</ol>
</li>
<li>How the agent <strong>acts (<span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>)</strong> in its current <strong>state (<span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>)</strong> is specified by its <strong>policy (<span><span class="MathJax_Preview">\pi(s)</span><script type="math/tex">\pi(s)</script></span>)</strong><ul>
<li>It can either be deterministic or stochastic<ol>
<li><strong>Deterministic policy</strong>: <span><span class="MathJax_Preview">a = \pi(s)</span><script type="math/tex">a = \pi(s)</script></span></li>
<li><strong>Stochastic policy</strong>: <span><span class="MathJax_Preview">\mathbb{P}_\pi [A=a \vert S=s] = \pi(a | s)</span><script type="math/tex">\mathbb{P}_\pi [A=a \vert S=s] = \pi(a | s)</script></span><ul>
<li>This is the proability of taking an action given the current state under the policy</li>
<li><span><span class="MathJax_Preview">\mathcal{A}</span><script type="math/tex">\mathcal{A}</script></span>: set of all actions</li>
<li><span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span>: set of all states</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>When the agent acts given its state under the <strong>policy (<span><span class="MathJax_Preview">\pi(a | s)</span><script type="math/tex">\pi(a | s)</script></span>)</strong>, the <strong>transition probability function <span><span class="MathJax_Preview">\mathcal{P}</span><script type="math/tex">\mathcal{P}</script></span></strong> determines the subsequent <strong>state (<span><span class="MathJax_Preview">s'</span><script type="math/tex">s'</script></span>)</strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{P}_{ss'}^a = \mathcal{P}(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a]</span><script type="math/tex">\mathcal{P}_{ss'}^a = \mathcal{P}(s' \vert s, a)  = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a]</script></span></li>
</ul>
</li>
<li>When the agent act based on its <strong>policy (<span><span class="MathJax_Preview">\pi(a | s)</span><script type="math/tex">\pi(a | s)</script></span>)</strong> and transited to the new state determined by the transition probability function <span><span class="MathJax_Preview">\mathcal{P}_{ss'}^a</span><script type="math/tex">\mathcal{P}_{ss'}^a</script></span> it gets a reward based on the <strong>reward function</strong> as a feedback<ul>
<li><span><span class="MathJax_Preview">\mathcal{R}_s^a = \mathbb{E} [\mathcal{R}_{t+1} \vert S_t = s, A_t = a]</span><script type="math/tex">\mathcal{R}_s^a = \mathbb{E} [\mathcal{R}_{t+1} \vert S_t = s, A_t = a]</script></span></li>
</ul>
</li>
<li>Rewards are short-term, given as feedback after the agent takes an action and transits to a new state. Summing all future rewards and discounting them would lead to our <strong>return <span><span class="MathJax_Preview">\mathcal{G}</span><script type="math/tex">\mathcal{G}</script></span></strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{G}_t = \sum_{i=0}^{N} \gamma^k \mathcal{R}_{t+1+i}</span><script type="math/tex">\mathcal{G}_t = \sum_{i=0}^{N} \gamma^k \mathcal{R}_{t+1+i}</script></span><ul>
<li><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span>, our discount factor which ranges from 0 to 1 (inclusive) reduces the weightage of future rewards allowing us to balance between short-term and long-term goals</li>
</ul>
</li>
</ul>
</li>
<li>With our <strong>return <span><span class="MathJax_Preview">\mathcal{G}</span><script type="math/tex">\mathcal{G}</script></span></strong>, we then have our <strong>state-value function <span><span class="MathJax_Preview">\mathcal{V}_{\pi}</span><script type="math/tex">\mathcal{V}_{\pi}</script></span></strong> (how good to stay in that state) and our <strong>action-value or q-value function <span><span class="MathJax_Preview">\mathcal{Q}_{\pi}</span><script type="math/tex">\mathcal{Q}_{\pi}</script></span></strong> (how good to take the action)<ul>
<li><span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s]</span><script type="math/tex">\mathcal{V}_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s]</script></span></li>
<li><span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s, \mathcal{A}_t = a]</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s, \mathcal{A}_t = a]</script></span></li>
<li>The advantage function is simply the difference between the two functions <span><span class="MathJax_Preview">\mathcal{A}_{\pi}(s, a) = \mathcal{Q}_{\pi}(s, a) - \mathcal{V}_{\pi}(s)</span><script type="math/tex">\mathcal{A}_{\pi}(s, a) = \mathcal{Q}_{\pi}(s, a) - \mathcal{V}_{\pi}(s)</script></span><ul>
<li>Seems useless at this stage, but this advantage function will be used in some key algorithms we are covering</li>
</ul>
</li>
</ul>
</li>
<li>Since our policy determines how our agent acts given its state, achieving an <strong>optimal policy <span><span class="MathJax_Preview">\pi_*</span><script type="math/tex">\pi_*</script></span></strong> would mean achieving optimal actions that is exactly what we want!</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Basic Categories of Approaches</p>
<p>We've covered state-value functions, action-value functions, model-free RL and model-based RL. They form general overarching categories of how we design our agent.</p>
<h4 id="categories-of-design">Categories of Design<a class="headerlink" href="#categories-of-design" title="Permanent link">&para;</a></h4>
<ol>
<li>State-value based: search for the optimal state-value function (goodness of action in the state)</li>
<li>Action-value based: search for the optimal action-value function (goodness of policy)</li>
<li>Actor-critic based: using both state-value and action-value function </li>
<li>Model based: attempts to model the environment to find the best policy</li>
<li>Model-free based: trial and error to optimize for the best policy to get the most rewards instead of modelling the environment explicitly</li>
</ol>
</div>
<h2 id="optimal-policy">Optimal Policy<a class="headerlink" href="#optimal-policy" title="Permanent link">&para;</a></h2>
<ul>
<li>Optimal policy <span><span class="MathJax_Preview">\pi_*</span><script type="math/tex">\pi_*</script></span> &rarr; optimal state-value and action-value functions &rarr; max return &rarr; argmax of value functions<ul>
<li><span><span class="MathJax_Preview">\pi_{*} = \arg\max_{\pi} \mathcal{V}_{\pi}(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s, a)</span><script type="math/tex">\pi_{*} = \arg\max_{\pi} \mathcal{V}_{\pi}(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s, a)</script></span></li>
</ul>
</li>
<li>To calculate argmax of value functions &rarr; we need max return <span><span class="MathJax_Preview">\mathcal{G}_t</span><script type="math/tex">\mathcal{G}_t</script></span> &rarr; need max sum of rewards <span><span class="MathJax_Preview">\mathcal{R}_s^a</span><script type="math/tex">\mathcal{R}_s^a</script></span></li>
<li>To get max sum of rewards <span><span class="MathJax_Preview">\mathcal{R}_s^a</span><script type="math/tex">\mathcal{R}_s^a</script></span> we will rely on the Bellman Equations.<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup></li>
</ul>
<h2 id="bellman-equation">Bellman Equation<a class="headerlink" href="#bellman-equation" title="Permanent link">&para;</a></h2>
<ul>
<li>Essentially, the Bellman Equation breaks down our value functions into two parts<ol>
<li>Immediate reward</li>
<li>Discounted future value function</li>
</ol>
</li>
<li>State-value function can be broken into:<ul>
<li><span><span class="MathJax_Preview">\begin{aligned}
    \mathcal{V}_{\pi}(s) &amp;= \mathbb{E}[\mathcal{G}_t \vert \mathcal{S}_t = s] \\
    &amp;= \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{R}_{t+2} + \gamma^2 \mathcal{R}_{t+3} + \dots \vert \mathcal{S}_t = s] \\
    &amp;= \mathbb{E} [\mathcal{R}_{t+1} + \gamma (\mathcal{R}_{t+2} + \gamma \mathcal{R}_{t+3} + \dots) \vert \mathcal{S}_t = s] \\
    &amp;= \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{G}_{t+1} \vert \mathcal{S}_t = s] \\
    &amp;= \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{V}_{\pi}(\mathcal{s}_{t+1}) \vert \mathcal{S}_t = s]
    \end{aligned}</span><script type="math/tex">\begin{aligned}
    \mathcal{V}_{\pi}(s) &= \mathbb{E}[\mathcal{G}_t \vert \mathcal{S}_t = s] \\
    &= \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{R}_{t+2} + \gamma^2 \mathcal{R}_{t+3} + \dots \vert \mathcal{S}_t = s] \\
    &= \mathbb{E} [\mathcal{R}_{t+1} + \gamma (\mathcal{R}_{t+2} + \gamma \mathcal{R}_{t+3} + \dots) \vert \mathcal{S}_t = s] \\
    &= \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{G}_{t+1} \vert \mathcal{S}_t = s] \\
    &= \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{V}_{\pi}(\mathcal{s}_{t+1}) \vert \mathcal{S}_t = s]
    \end{aligned}</script></span></li>
</ul>
</li>
<li>Action-value function can be broken into:<ul>
<li><span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{Q}_{\pi}(\mathcal{s}_{t+1}, \mathcal{a}_{t+1}) \vert \mathcal{S}_t = s, \mathcal{A} = a]</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathbb{E} [\mathcal{R}_{t+1} + \gamma \mathcal{Q}_{\pi}(\mathcal{s}_{t+1}, \mathcal{a}_{t+1}) \vert \mathcal{S}_t = s, \mathcal{A} = a]</script></span></li>
</ul>
</li>
</ul>
<h2 id="key-recap-on-value-functions">Key Recap on Value Functions<a class="headerlink" href="#key-recap-on-value-functions" title="Permanent link">&para;</a></h2>
<ul>
<li><span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s]</span><script type="math/tex">\mathcal{V}_{\pi}(s) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s]</script></span><ul>
<li>State-value function: tells us how good to be in that state</li>
</ul>
</li>
<li><span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s, \mathcal{A}_t = a]</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathbb{E}_{\pi}[\mathcal{G}_t \vert \mathcal{S}_t = s, \mathcal{A}_t = a]</script></span><ul>
<li>Action-value function: tells us how good to take actions given state</li>
</ul>
</li>
</ul>
<h2 id="bellman-expectation-equations">Bellman Expectation Equations<a class="headerlink" href="#bellman-expectation-equations" title="Permanent link">&para;</a></h2>
<ul>
<li>Now we can move from Bellman Equations into Bellman Expectation Equations</li>
<li><strong>Basic: State-value function <span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s)</span><script type="math/tex">\mathcal{V}_{\pi}(s)</script></span></strong><ol>
<li>Current state <span><span class="MathJax_Preview">\mathcal{S}</span><script type="math/tex">\mathcal{S}</script></span></li>
<li>Multiple possible actions determined by stochastic policy <span><span class="MathJax_Preview">\pi(a | s)</span><script type="math/tex">\pi(a | s)</script></span></li>
<li>Each possible action is associated with a action-value function <span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a)</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a)</script></span> returning a value of that particular action</li>
<li>Multiplying the possible actions with the action-value function and summing them gives us an indication of how good it is to be in that state<ul>
<li><strong>Final equation: <span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{Q}(s, a)</span><script type="math/tex">\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{Q}(s, a)</script></span></strong></li>
<li><strong>Loose intuitive interpretation</strong>: <ul>
<li>state-value = sum(policy determining actions * respective action-values)</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><strong>Basic: Action-value function <span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a)</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a)</script></span></strong><ol>
<li>With a list of possible multiple actions, there is a list of possible subsequent states <span><span class="MathJax_Preview">s'</span><script type="math/tex">s'</script></span> associated with:<ol>
<li>state value function <span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s')</span><script type="math/tex">\mathcal{V}_{\pi}(s')</script></span> </li>
<li>transition probability function <span><span class="MathJax_Preview">\mathcal{P}_{ss'}^a</span><script type="math/tex">\mathcal{P}_{ss'}^a</script></span> determining where the agent could land in based on the action</li>
<li>reward <span><span class="MathJax_Preview">\mathcal{R}_s^a</span><script type="math/tex">\mathcal{R}_s^a</script></span> for taking the action</li>
</ol>
</li>
<li>Summing the reward and the transition probability function associated with the state-value function gives us an indication of how good it is to take the actions given our state<ul>
<li><strong>Final equation: <span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s')</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s')</script></span></strong></li>
<li><strong>Loose intuitive interpretation</strong>: <ul>
<li>action-value = reward + sum(transition outcomes determining states * respective state-values)</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li><strong>Expanded functions (substitution)</strong><ul>
<li>Substituting action-value function into the <strong>state-value function</strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s'))</span><script type="math/tex">\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s'))</script></span></li>
</ul>
</li>
<li>Substituting  state-value function into <strong>action-value function</strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' | s') \mathcal{Q}(s', a')</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' | s') \mathcal{Q}(s', a')</script></span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="bellman-optimality-equations">Bellman Optimality Equations<a class="headerlink" href="#bellman-optimality-equations" title="Permanent link">&para;</a></h2>
<ul>
<li>Remember optimal policy <span><span class="MathJax_Preview">\pi_*</span><script type="math/tex">\pi_*</script></span> &rarr; optimal state-value and action-value functions &rarr; argmax of value functions<ul>
<li><span><span class="MathJax_Preview">\pi_{*} = \arg\max_{\pi} \mathcal{V}_{\pi}(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s, a)</span><script type="math/tex">\pi_{*} = \arg\max_{\pi} \mathcal{V}_{\pi}(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s, a)</script></span></li>
</ul>
</li>
<li>Finally with Bellman Expectation Equations derived from Bellman Equations, we can derive the equations for the argmax of our value functions</li>
<li><strong>Optimal state-value function</strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{V}_*(s) = \arg\max_{\pi} \mathcal{V}_{\pi}(s)</span><script type="math/tex">\mathcal{V}_*(s) = \arg\max_{\pi} \mathcal{V}_{\pi}(s)</script></span></li>
<li>Given <span><span class="MathJax_Preview">\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s'))</span><script type="math/tex">\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s'))</script></span></li>
<li>We have <span><span class="MathJax_Preview">\mathcal{V}_*(s) = \max_{a \in \mathcal{A}} (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{*}(s')))</span><script type="math/tex">\mathcal{V}_*(s) = \max_{a \in \mathcal{A}} (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{*}(s')))</script></span></li>
</ul>
</li>
<li><strong>Optimal action-value function</strong><ul>
<li><span><span class="MathJax_Preview">\mathcal{Q}_*(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s)</span><script type="math/tex">\mathcal{Q}_*(s) = \arg\max_{\pi} \mathcal{Q}_{\pi}(s)</script></span></li>
<li>Given <span><span class="MathJax_Preview">\mathcal{Q}_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' | s') \mathcal{Q}(s', a')</span><script type="math/tex">\mathcal{Q}_{\pi}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' | s') \mathcal{Q}(s', a')</script></span></li>
<li>We have <span><span class="MathJax_Preview">\mathcal{Q}_{*}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a max_{a' \in \mathcal{A}} \mathcal{Q}_{*}(s', a')</span><script type="math/tex">\mathcal{Q}_{*}(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a max_{a' \in \mathcal{A}} \mathcal{Q}_{*}(s', a')</script></span></li>
</ul>
</li>
</ul>
<h2 id="optimal-action-value-and-state-value-functions">Optimal Action-value and State-value functions<a class="headerlink" href="#optimal-action-value-and-state-value-functions" title="Permanent link">&para;</a></h2>
<ol>
<li>If the entire environment is known, such that we know our reward function and transition probability function, then we can solve for the optimal action-value and state-value functions via <strong>Dynamic Programming</strong> like<ul>
<li>Policy evaluation, policy improvement, and policy iteration</li>
</ul>
</li>
<li>However, typically we don't know the environment entirely then there is not closed form solution in getting optimal action-value and state-value functions. Hence, we need other iterative approaches like<ol>
<li><strong>Monte-Carlo methods</strong></li>
<li><strong>Temporal difference learning</strong> (model-free and learns with episodes)<ol>
<li>On-policy TD: SARSA</li>
<li>Off-policy TD: Q-Learning and Deep Q-Learning (DQN)</li>
</ol>
</li>
<li><strong>Policy gradient</strong><ul>
<li>REINFORCE</li>
<li>Actor-Critic</li>
<li>A2C/A3C</li>
<li>ACKTR</li>
<li>PPO</li>
<li>DPG</li>
<li>DDPG (DQN + DPG)</li>
</ul>
</li>
</ol>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Closed form solution</p>
<p>If there is a closed form solution, then the variables' values can be obtained with a finite number of mathematical operations (for example add, subtract, divide, and multiply).</p>
<p>For example, solving <span><span class="MathJax_Preview">2x = 8 - 6x</span><script type="math/tex">2x = 8 - 6x</script></span> would yield <span><span class="MathJax_Preview">8x = 8</span><script type="math/tex">8x = 8</script></span> by adding <span><span class="MathJax_Preview">6x</span><script type="math/tex">6x</script></span> on both sides of the equation and finally yielding the value of <span><span class="MathJax_Preview">x=1</span><script type="math/tex">x=1</script></span> by dividing both sides of the equation by <span><span class="MathJax_Preview">8</span><script type="math/tex">8</script></span>. </p>
<p>These finite 2 steps of mathematical operations allowed us to solve for the value of x as the equation has a closed-form solution.</p>
<p>However, many cases in deep learning and reinforcement learning there are no closed-form solutions which requires all the iterative methods mentioned above.</p>
</div>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Bellman, R. A Markovian Decision Process. Journal of Mathematics and Mechanics. 1957.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Matthew J. Hausknecht and Peter Stone. <a href="https://arxiv.org/abs/1507.06527">Deep Recurrent Q-Learning for Partially Observable MDPs</a>. 2015.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>R Bellman. On the Theory of Dynamic Programming. Proceedings of the National Academy of Sciences. 1952.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
              
              
                


  <h2 id="__comments">Comments</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/",this.page.identifier="deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/"};window.addEventListener("load",function(){var e=document,i=e.createElement("script");i.src="//deep-learning-wizard.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)})</script>

              
            </article>
          </div>
        </div>
      </main>
      
        

<!-- Application footer -->
<footer class="md-footer">

  <!-- Link to previous and/or next page -->
  
    <div class="md-footer-nav">
      <nav
        class="md-footer-nav__inner md-grid"
        aria-label="Footer"
      >

        <!-- Link to previous page -->
        
          <a
            href="../supervised_to_rl/"
            title="Supervised Learning to Reinforcement Learning (RL)"
            class="md-footer-nav__link md-footer-nav__link--prev"
            rel="prev"
          >
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Supervised Learning to Reinforcement Learning (RL)
              </div>
            </div>
          </a>
        

        <!-- Link to next page -->
        
          <a
            href="../dynamic_programming_frozenlake/"
            title="Dynamic Programming"
            class="md-footer-nav__link md-footer-nav__link--next"
            rel="next"
          >
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Dynamic Programming
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  

  <!-- Further information -->
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">

      <!-- Copyright and theme information -->
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2021 Deep Learning Wizard by Ritchie Ng
          </div>
        
        proudly an
        <a href="http://www.nvidia.com/page/home.html">NVIDIA Inception Partner</a> and supported by the
        <a href="https://aws.amazon.com/activate/">Amazon AWS Activate Programme</a> by
        <a href="https://www.ritchieng.com/">
          Ritchie Ng</a>
        </a>
      </div>

      <!-- Social links -->
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ritchieng" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.facebook.com/DeepLearningWizard/" target="_blank" rel="noopener" title="www.facebook.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/company/deeplearningwizard/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../../assets/javascripts/vendor.93c04032.min.js"></script>
      <script src="../../../assets/javascripts/bundle.83e5331e.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "../../..",
          features: ['navigation.tabs', 'instant'],
          search: Object.assign({
            worker: "../../../assets/javascripts/worker/search.8c7e0a7e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>